## SQLite Lab

![4 17-etl-sqlite-RAW](https://github.com/nogibjj/sqlite-lab/assets/58792/b39b21b4-ccb4-4cc4-b262-7db34492c16d)

[![CI](https://github.com/nogibjj/Rishika_Randev_MiniProject_1/actions/workflows/hello.yml/badge.svg)](https://github.com/nogibjj/Rishika_Randev_MiniProject_1/actions/workflows/hello.yml)

# Rishika Randev's Pandas Descriptive Script for IDS706 Week 5

## ☑️ Requirements (Mini Project 5):
1. Connect to a SQL database
2. Perform CRUD operations
3. Write at least two different SQL queries

## ☑️ The Dataset
The dataset used in this project shows diet, physical activity, and nutrition data from the behaviorial risk factors survey across the US in 2023. It is published by the U.S. Department of Health & Human Services and freely available through data.gov at this link: https://catalog.data.gov/dataset/nutrition-physical-activity-and-obesity-behavioral-risk-factor-surveillance-system.

## ☑️ Steps
1. Prepare the necesary configuration files like the Dockerfile, devcontainer.json, Makefile, requirements.txt, and main.yml for GitHub Actions integration. Ensure that the requirements.txt lists all necessary packages (for example, matplotlib for visualizing and pandas for data manipulation).
2. Create a main.py script with two functions--
   * generate_summary_stats(csv): reads in any csv file passed to it into a pandas dataframe and then generates summary statistics (mean, median, mode, standard deviation) for its columns.
   * generate_data_viz(csv): reads in the csv file, creates a scatterplot of Hours Studied vs. Exam Score using matplotlib, and saves it as a png file (performance.png).
3. Create a test_main.py script with two functions--
   * test_generate_summary_stats(csv): calls generate_summary_stats() using the student performance factors csv file to validate a few of the sample statistics generated by this function.
     
   ![Sample Stats](https://github.com/user-attachments/assets/54a6c401-c230-46b9-948d-0e2929d952f4)
   * test_generate_data_viz(csv): calls generate_data_viz() using the student performance factors csv file in order to produce the below scatterplot.
     
     ![Visualization](performance.png)
4. Create a [Jupyter Notebook](summary.ipynb) with the same code as the main.py script to easily show the outputs of the descriptive statistics and data visualization.
5. Using the main.yml file, set up a GitHub Actions workflow so that every time changes are pushed to the repository, all of the Makefile commands are run to ensure that new code is properly formatted using Black, linted using Ruff, and tested using Pytest. A pdf or markdown summary file can also be generated through GH Actions (or it can be manually pushed to the repository, by converting the Jupyter notebook to html / pdf).

  - `make install`
    
    ![requirements](https://github.com/user-attachments/assets/0a88d102-f326-4961-83ea-ce40d5930178)

  - `make format`
    
    ![formatting](https://github.com/user-attachments/assets/87809dd7-7128-44be-9dfb-0f3528d2afde8)

  - `make lint`
    
    ![linting](https://github.com/user-attachments/assets/e186bb79-fe4d-4633-a04b-22f7b8d8bfb1)

  - `make test`
  
    ![testing](https://github.com/user-attachments/assets/888bdf3d-fad7-42b8-9d36-985d7625a718)

## ☑️ Summary File
The outputs of the descriptive statistics and visualization showing Hours Studied vs. Exam Scores are captured in this [pdf file](summary.pdf).

### Lab:

* Use an AI Assistant, but use a different one then you used from a previous lab (Anthropic's Claud, Bard, Copilot, CodeWhisperer, Colab AI, etc)
* ETL-Query:  [E] Extract a dataset from URL, [T] Transform, [L] Load into SQLite Database and [Q] Query
For the ETL-Query lab:
* [E] Extract a dataset from a URL like Kaggle or data.gov. JSON or CSV formats tend to work well.
* [T] Transform the data by cleaning, filtering, enriching, etc to get it ready for analysis.
* [L] Load the transformed data into a SQLite database table using Python's sqlite3 module.
* [Q] Write and execute SQL queries on the SQLite database to analyze and retrieve insights from the data.

#### Tasks:

* Fork this project and get it to run
* Make the query more useful and not a giant mess that prints to screen
* Convert the main.py into a command-line tool that lets you run each step independantly
* Fork this project and do the same thing for a new dataset you choose
* Make sure your project passes lint/tests and has a built badge
* Include an architectural diagram showing how the project works

#### Reflection Questions

* What challenges did you face when extracting, transforming, and loading the data? How did you overcome them?
* What insights or new knowledge did you gain from querying the SQLite database?
* How can SQLite and SQL help make data analysis more efficient? What are the limitations?
* What AI assistant did you use and how did it compare to others you've tried? What are its strengths and weaknesses?
* If you could enhance this lab, what would you add or change? What other data would be interesting to load and query?

##### Challenge Exercises

* Add more transformations to the data before loading it into SQLite. Ideas: join with another dataset, aggregate by categories, normalize columns.
* Write a query to find correlated fields in the data. Print the query results nicely formatted.
* Create a second table in the SQLite database and write a join query with the two tables.
* Build a simple Flask web app that runs queries on demand and displays results.
* Containerize the application using Docker so the database and queries can be portable


